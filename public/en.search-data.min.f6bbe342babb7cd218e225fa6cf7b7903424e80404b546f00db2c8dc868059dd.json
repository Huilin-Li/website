[{"id":0,"href":"/posts/blog/one/","title":"The periodic table, electron shells, and orbitals","section":"Blog","content":"Differences in chemical reactivity between elements are based on the number and spatial distribution of their electrons.\nThe Bohr model shows the atom as a central nucleus containing protons and neutrons, with the electrons in circular electron shells at specific distances from the nucleus, similar to planets orbiting around the sun. Each electron shell has a different energy level, with those shells closest to the nucleus being lower in energy than those farther from the nucleus.\n"},{"id":1,"href":"/docs/project/binderdesign/truncation/","title":"truncation","section":"Binder Design","content":"\rtarget.pdb preprocessing\r#\rAssuming we have a target protein target.pdb, and we want to create a binder to target.pdb. The first step is preprocessing target.pdb. Generally spearking, there are two steps we need to think of.\nTruncating target.pdb\r#\rTruncating target protein helps a lot in reducing the stress of computing. For example, we could remove AAs with very low pLDDT or AAs who are far away from the place we want binders to bind with.\ndef residue_remove(pdbfile, remove_ids, outputName): \u0026#34;\u0026#34;\u0026#34; Remove residues based on positions. Assumption: pdbfile only has A chain \u0026#34;\u0026#34;\u0026#34; residue_to_remove = [] pdb_io = PDB.PDBIO() pdb_parser = PDB.PDBParser() structure = pdb_parser.get_structure(\u0026#34; \u0026#34;, pdbfile) model = structure[0] chain = model[\u0026#34;A\u0026#34;] # assuming pdbfile only has A chain for residue in chain: id = residue.id if id[1] in remove_ids: residue_to_remove.append(residue.id) for residue in residue_to_remove: chain.detach_child(residue) pdb_io.set_structure(structure) pdb_io.save(outputName) return # truncating ids truncating_ids = [1,2,3] # remove some AAs residue_remove(pdbfile=\u0026#34;target.pdb\u0026#34;, remove_ids=truncating_ids, outputName=\u0026#34;truncation.pdb\u0026#34;) This step can also be achieved in PyMol.\nHow many AAs we can remove from our target protein?\nAt the very beginning, I was thinking removing all useless AAs. For example, I was creating a binder to a transmembrane protein, and I only left the part of protein which is located at the outside of the whole protein. Unfornutately, this part of protein I aim to create binders to bind with is too \u0026ldquo;thin\u0026rdquo; to lead design binders correctly.\nIf the \u0026ldquo;target\u0026rdquo; protein you feed into the RFdiffusion model is too small, ppi.hotspot_res argument becomes ineffective. Let\u0026rsquo;s imagine the \u0026ldquo;target\u0026rdquo; protein we feed into the RFdiffusion model is a paper, and ppi.hotspot_res is located somewhere in this paper. Since this \u0026ldquo;target\u0026rdquo; protein (the paper) is too thin, RFdiffusion model feels confused about which side (above or below the paper ) the ppi.hotspot_res points to.\nMy suggestion would be to try some different trunctations and pick one that can approcimately balance the computation stress and correct leading of bidner design.\nReset AAs\u0026rsquo; residues numbers in truncation.pdb\r#\rNormally, after removing some AAs, the residues number in truncation.pdb becomes discontinuous, and we want to concatenate them again and update their residue number with continuous numbers.\ndef update_resnum(pdbfile): \u0026#34;\u0026#34;\u0026#34; 🌟 CHANGE RESIDUE NUMBER Assumption: pdbfile only has A chain \u0026#34;\u0026#34;\u0026#34; pdb_io = PDB.PDBIO() pdb_parser = PDB.PDBParser() structure = pdb_parser.get_structure(\u0026#34; \u0026#34;, pdbfile) model = structure[0] chain = model[\u0026#34;A\u0026#34;] print(len([i for i in chain.get_residues()])) new_resnums = list(range(1, 1+len([i for i in chain.get_residues()]))) for i, residue in enumerate(chain.get_residues()): res_id = list(residue.id) res_id[1] = new_resnums[i] residue.id = tuple(res_id) pdb_io.set_structure(structure) pdb_io.save(pdbfile.split(\u0026#39;.pdb\u0026#39;)[0] + \u0026#39;_update_resnum.pdb\u0026#39;) return "},{"id":2,"href":"/docs/project/binderdesign/design/","title":"design binders via RFdiffusion and ProteinMPNN-FastRelax","section":"Binder Design","content":"In my use case, firstly, I tried to de novo design binders. However, I found that designed binders are limited to proteins with only one helix. Then, I changed to design binders based on scaffolds. The design process and scripts I used a lot are as follows.\nThree needed designing environments\r#\rThanks a lot to the great work, RFdiffusion and ProteinMPNN-FastRelax, as well as AlphaFold2, who give us great ways to design proteins.\nRFdiffuion for desigining protein backbones, under SE3nv environment. ProteinMPNN-FastRelax for desiging protein sequences, under dl_binder_design environment. AlphaFold2 for evaluating designed proteins, under af2_binder_design environment. We will design binders under these three environments: SE3nv environment, dl_binder_design environment, af2_binder_design environment. Install these three environments\rMake sure you have high performance computating envrionment with CUDA, and you can access GPU and internet. Their installations can automatically install dependencied packages that fit the GPU environment which you have to use for the heavy computation.\rQuickly go through the whole process\r#\rLet us say that we want to design binders for target.pdb, and our current directories tree is\n├── dl_binder_design\r│ ├── af2_initial_guess\r│ │ ├── predict.py\r│ ├── mpnn_fr\r│ │ ├── dl_interface_design.py\r├── mydesigns\r│ ├── target\r│ │ └── target.pdb\r├── RFdiffusion\r│ ├── rfdiffusion\r│ ├── scripts\r│ │ └── run_inference.py\r│ ├── helper_scripts\r│ │ └── make_secstruc_adj.py 1️⃣ get secondary structure and block adjacency information script get_adj_secstruct.slm\r#\rWe will provide these two .pt files (target_adj.pt and target_ss.pt) to the scaffold-based binder design model.\n#!/bin/bash #SBATCH -q gpu-huge #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH -p GPU-name #SBATCH --gres=gpu:1 #SBATCH --mem=50G #SBATCH -o %j.out source ~/miniconda3/etc/profile.d/conda.sh conda activate SE3nv cd /(root)/mydesigns python3 ../RFdiffusion/helper_scripts/make_secstruc_adj.py --input_pdb ./target/target.pdb --out_dir ./target_adj_secstruct Then, our directories tree is updated as:\n├── dl_binder_design\r│ ├── af2_initial_guess\r│ │ ├── predict.py\r│ ├── mpnn_fr\r│ │ ├── dl_interface_design.py\r├── mydesigns\r│ ├── get_adj_secstruct.slm\r│ ├── target\r│ │ └── target.pdb\r│ ├── target_adj_secstruct\r│ │ └── target_adj.pt\r│ │ └── target_ss.pt\r├── RFdiffusion\r│ ├── rfdiffusion\r│ ├── scripts\r│ │ └── run_inference.py\r│ ├── helper_scripts\r│ │ └── make_secstruc_adj.py 2️⃣ backbones design script bb.slm\r#\rI use array-job to parallelly execute different ppi.hotspot_res arguments simultaneously. For example, I have 10 ppi.hotspot_res arguments to test, so my paraID.txt is like:\nmy `paraID.txt` file\rand I setup 10 jobs (#SBATCH -a 1-10) for each `ppi.hotspot_res`. Each `ppi.hotspot_res` will design inference.num_designs=10000 binder backbones.\r#!/bin/bash #SBATCH -q gpu-huge #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH -p GPU-name #SBATCH --gres=gpu:1 #SBATCH --mem=50G #SBATCH -J array-job\t#SBATCH -a 1-10 #SBATCH -o backbone_10000.%A.%a.log id_list=\u0026#34;./paraID.txt\u0026#34; id=`head -n $SLURM_ARRAY_TASK_ID $id_list | tail -n 1` source ~/miniconda3/etc/profile.d/conda.sh conda activate SE3nv cd /(root)/mydesigns python3 ../RFdiffusion/scripts/run_inference.py \\ scaffoldguided.target_path=./target/target.pdb \\ inference.output_prefix=backbones_OUT/ \\ scaffoldguided.scaffoldguided=True \\ $id scaffoldguided.target_pdb=True \\ scaffoldguided.target_ss=./slightly_truncation_adj_secstruct/slightly_truncation_ss.pt \\ scaffoldguided.target_adj=./slightly_truncation_adj_secstruct/slightly_truncation_adj.pt \\ scaffoldguided.scaffold_dir=../RFdiffusion/examples/ppi_scaffolds/ \\ scaffoldguided.mask_loops=False \\ inference.num_designs=10000 \\ denoiser.noise_scale_ca=0 \\ denoiser.noise_scale_frame=0 Untar 1000 scaffold templates\rDon't forget to untar the provided 1000 scaffold templates (RFdiffusion/examples/ppi_scaffolds_subset.tar.gz). In ppi_scaffolds_subset.tar.gz, there is ppi_scaffolds folder and we will use it in our backbone designing script.\rNow, our directories tree becomes:\n├── dl_binder_design\r│ ├── af2_initial_guess\r│ │ ├── predict.py\r│ ├── mpnn_fr\r│ │ ├── dl_interface_design.py\r├── mydesigns\r│ ├── get_adj_secstruct.slm\r│ ├── paraID.txt\r│ ├── bb.slm\r│ ├── target\r│ │ └── target.pdb\r│ ├── target_adj_secstruct\r│ │ └── target_adj.pt\r│ │ └── target_ss.pt\r│ ├── backbones_OUT\r│ │ ├── traj\r│ │ ├── A28-A25-A29-A26-A63_0.pdb\r│ │ ├── A28-A25-A29-A26-A63_0.trb\r│ │ ├── ...\r├── RFdiffusion\r│ ├── rfdiffusion\r│ ├── scripts\r│ │ └── run_inference.py\r│ ├── helper_scripts\r│ │ └── make_secstruc_adj.py Add ppi_scaffolds argument in backbone name\rIn RFdiffusion/scripts/run_inference.py, I modify some codes a little bit, so that, the backbone pdb file name will contain the `ppi_scaffolds` argument.\r\u0026lt;img src=\u0026quot;../bioIMG/mdf1.PNG\u0026quot; /\u0026gt;\r\u0026lt;img src=\u0026quot;../bioIMG/mdf2.PNG\u0026quot; /\u0026gt;\rTherefore, one of inference.num_designs=10000 binder backbones for ppi.hotspot_res=[A28-A25-A29-A26-A63] argument is A28-A25-A29-A26-A63_0.pdb.\r3️⃣ sequence design and binder assessment script mpnn_af.slm\r#\rPerform a filtering step\rAt this moment, we could perform a filtering step after we get all potential binder backbones. For example, I only want backbones (orange cross) in one side (green circle). We could remove other useless backbones, which can help a lot in reducing the computation stress.\r\u0026lt;img src=\u0026quot;../bioIMG/eg.png\u0026quot; /\u0026gt;\rLet\u0026rsquo;s say that we finally select 1000 favourite backbones, and we go to design binders based on these 1000 backbones. Normally, I will split favourite backbones into multiple folders, so I could parallelly design binders in each folder simultaneously. For example, in order to speed up the binder design process, I split the 1000 favourite backbones into 5 folders. After that, I could parallely execute #SBATCH -a 1-5 jobs simultaneously.\n# sele_list contains names of 1000 selected backbones, # such as A28-A25-A29-A26-A63_0.pdb folder_size = 100 # how many pdb files you want at most to be grouped in one folder chunks = [sele_list[x:x+folder_size] for x in range(0, len(sele_list), folder_size)] # len(chunks)=5 for i in range(len(chunks)): chunk = chunks[i] newpath = \u0026#34;../mydesigns/select_1000_mpnn_af/folder\u0026#34; + str(i) if not os.path.exists(newpath): os.makedirs(newpath) for j in chunk: src = \u0026#34;../mydesigns/backbones_OUT/\u0026#34; + j dst = \u0026#34;../mydesigns/select_1000_mpnn_af/folder\u0026#34; + str(i) + \u0026#34;/\u0026#34; + j shutil.copyfile(src, dst) #!/bin/bash #SBATCH -q gpu-huge #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH -p GPU-name #SBATCH --gres=gpu:1 #SBATCH --mem=50G #SBATCH -J array-job\t#SBATCH -a 1-5 #SBATCH -o mpnn_af_1000.%A.%a.log id_list=\u0026#34;./folderID.txt\u0026#34; id=`head -n $SLURM_ARRAY_TASK_ID $id_list | tail -n 1` cd /(root)/mydesigns/target_1000_mpnn_af cd $id source ~/miniconda3/etc/profile.d/conda.sh conda activate proteinmpnn_binder_design silentfrompdbs *.pdb \u0026gt; silent.silent python ../../../BinderDesign/dl_binder_design/mpnn_fr/dl_interface_design.py -silent silent.silent -outsilent seq.silent conda deactivate conda activate af2_binder_design python ../../../BinderDesign/dl_binder_design/af2_initial_guess/predict.py -silent seq.silent -outsilent af2.silent -scorefilename score.sc Now our directories tree is like:\n├── dl_binder_design\r│ ├── af2_initial_guess\r│ │ ├── predict.py\r│ ├── mpnn_fr\r│ │ ├── dl_interface_design.py\r├── mydesigns\r│ ├── get_adj_secstruct.slm\r│ ├── paraID.txt\r│ ├── bb.slm\r│ ├── target\r│ │ └── target.pdb\r│ ├── target_adj_secstruct\r│ │ └── target_adj.pt\r│ │ └── target_ss.pt\r│ ├── backbones_OUT\r│ │ ├── traj\r│ │ ├── A28-A25-A29-A26-A63_0.pdb\r│ │ ├── A28-A25-A29-A26-A63_0.trb\r│ │ ├── ...\r│ ├── select_1000_mpnn_af\r│ │ ├── folder0\r│ │ │ └── A28-A25-A29-A26-A63_0.pdb\r│ │ │ └── ...\r│ │ │ └── af2.silent\r│ │ │ └── check.point\r│ │ │ └── score.sc\r│ │ │ └── seq.silent\r│ │ │ └── seq.silent.idx\r│ │ │ └── silent.silent\r│ │ │ └── silent.silent.idx\r│ │ ├── folder1\r│ │ ├── ...\r│ │ ├── folderID.txt\r│ │ ├── mpnn_af.slm\r├── RFdiffusion\r│ ├── rfdiffusion\r│ ├── scripts\r│ │ └── run_inference.py\r│ ├── helper_scripts\r│ │ └── make_secstruc_adj.py 4️⃣ Get designed binders\r#\rIn each mydesigns/select_1000_mpnn_af/folder, af2.silent has our designed binders. And, we will extract their .pdb files from the .silent file by (for example)\n#!/bin/bash #SBATCH -q gpu-huge #SBATCH --nodes=1 #SBATCH --ntasks-per-node=8 #SBATCH -p GPU-name #SBATCH --gres=gpu:1 #SBATCH --mem=50G cd /(root)/mydesigns/select_1000_mpnn_af/folder0 source ~/miniconda3/etc/profile.d/conda.sh conda activate proteinmpnn_binder_design silentextract af2.silent Our final directories tree is like:\n├── dl_binder_design\r│ ├── af2_initial_guess\r│ │ ├── predict.py\r│ ├── mpnn_fr\r│ │ ├── dl_interface_design.py\r├── mydesigns\r│ ├── get_adj_secstruct.slm\r│ ├── paraID.txt\r│ ├── bb.slm\r│ ├── target\r│ │ └── target.pdb\r│ ├── target_adj_secstruct\r│ │ └── target_adj.pt\r│ │ └── target_ss.pt\r│ ├── backbones_OUT\r│ │ ├── traj\r│ │ ├── A28-A25-A29-A26-A63_0.pdb\r│ │ ├── A28-A25-A29-A26-A63_0.trb\r│ │ ├── ...\r│ ├── select_1000_mpnn_af\r│ │ ├── folder0\r│ │ │ └── A28-A25-A29-A26-A63_0.pdb\r│ │ │ └── A28-A25-A29-A26-A63_0_dldesign_0_cycle1_af2pred.pdb\r│ │ │ └── ...\r│ │ │ └── af2.silent\r│ │ │ └── check.point\r│ │ │ └── score.sc\r│ │ │ └── seq.silent\r│ │ │ └── seq.silent.idx\r│ │ │ └── silent.silent\r│ │ │ └── silent.silent.idx\r│ │ ├── folder1\r│ │ ├── ...\r│ │ ├── folderID.txt\r│ │ ├── mpnn_af.slm\r├── RFdiffusion\r│ ├── rfdiffusion\r│ ├── scripts\r│ │ └── run_inference.py\r│ ├── helper_scripts\r│ │ └── make_secstruc_adj.py Each file with _cycle1_af2pred.pdb in name is our expected designed binders.\n"},{"id":3,"href":"/posts/blog/pymol/","title":"PyMol commands","section":"Blog","content":" select residues based on residue number select nterm, resi 42+74+43+56+39 mutate one amino acid cmd.wizard(\u0026#34;mutagenesis\u0026#34;) cmd.fetch(\u0026#34;target.pdb\u0026#34;) cmd.get_wizard().set_mode(\u0026#34;LEU\u0026#34;) cmd.get_wizard().do_select(\u0026#34;chain A and resid 22\u0026#34;) cmd.get_wizard().apply() cmd.save(\u0026#34;mutated_target.pdb\u0026#34;) "},{"id":4,"href":"/posts/blog/python/","title":"Helpul python/pandas scripts","section":"Blog","content":"So\u0026hellip;, I find I used these codes a lot, and they are very helpful for effectivity.\nAssign files into different folders\r#\rAssuming I have 230 files in db_folder, and I want to copy these 230 files into different folders (folder0, folder1 and so on) with at least 50 files in each folder.\nfiles_list contains names of these 230 files, like files_list=[\u0026quot;file1\u0026quot;, \u0026quot;file2\u0026quot;, ..., \u0026quot;file230\u0026quot;]\nimport shutil chunks = [files_list[x:x+50] for x in range(0, len(files_list), 50)] # len(chunks) = 5 for i in range(53): chunk = chunks[i] newpath = \u0026#34;db_folder/folder\u0026#34; + str(i) if not os.path.exists(newpath): os.makedirs(newpath) for j in chunk: src = \u0026#34;db_folder/\u0026#34; + j dst = \u0026#34;db_folder/folder\u0026#34; + str(i) + \u0026#34;/\u0026#34; + j shutil.copyfile(src, dst) Copy files to a new folder\r#\rimport shutil for i in files_list: src = \u0026#34;old_folder/\u0026#34; + i dst = \u0026#34;new_folder/\u0026#34; + i shutil.copyfile(src, dst) Remove legend in Plotly\r#\rfig.update_layout(showlegend=False) "},{"id":5,"href":"/docs/project/binderdesign/pae_score/","title":"pae_interaction score","section":"Binder Design","content":"As RFdiffusion states, we expect a binder whose pae_interaction score is lower than 10. However, we should also have a look at other scores, such as pLDDT, rmsd.\ncheck_n = 6 folder_nam = [\u0026#34;folder\u0026#34;+str(i) for i in range(check_n)] score_paths = [\u0026#34;../MyBinderWork/ThreeThoudand_mpnn_af/\u0026#34;+nam+\u0026#34;/score.sc\u0026#34; for nam in folder_nam] ALL_DF = [] for p in score_paths: df = pd.read_csv(p, delim_whitespace=True) ALL_DF.append(df) SCORE_df = pd.concat(ALL_DF)[[\u0026#34;binder_aligned_rmsd\u0026#34;, \u0026#34;pae_binder\u0026#34;, \u0026#34;pae_interaction\u0026#34;, \u0026#34;plddt_binder\u0026#34;, \u0026#34;description\u0026#34;]] An example of one of my experiments\r"},{"id":6,"href":"/docs/project/binderdesign/filtering/","title":"backbones filtering","section":"Binder Design","content":"In design binders via RFdiffusion and ProteinMPNN-FastRelax, at the beginning of sequence design and binder assessment script mpnn_af.slm, we could perform a fitering step to remove useless backnones.\nIn my opinion, this is a very heloful step, because it greatly increase the speed and efficiency of desigining reasonable binders. For example, in my target.pdb, I only expect designed binders could bind with the outside part of the target.pdb (see fig.1).\nfig.1 `target.pdb` and atoms where binders should bind with\rfig.2 distribution of all designed backbone centers\r"},{"id":7,"href":"/docs/project/binderdesign/notes/","title":"notes","section":"Binder Design","content":" RFdiffusion is better at designing binders for target proteins that are already existing or have been verified by experiments. If the target protein is predcited by AI models, and has not been verified by experiments, the probability of designing a potential binder for this target will be reduced. The number of designed binders should be huge, in order to pick potential candidates as many as possible. "},{"id":8,"href":"/posts/blog/proteinstruture/","title":"4 levels of protein structure","section":"Blog","content":"There are four levels of protein structure: primary, secondary, tertiary and quaternary.\nPrimary Structure.\r#\rA protein’s primary structure is the unique sequence of amino acids in each polypeptide chain that makes up the protein.\nfasta file sequence of 20 amino acids Secondary Structure\r#\rReference:\r#\r3.9: Proteins - Protein Structure "},{"id":9,"href":"/posts/blog/overleaf/","title":"All about Latex and Overleaf","section":"Blog","content":"\\documentclass{article}\r\\usepackage[utf8]{inputenc}\r\\usepackage{tcolorbox}\r\\newtcbox{\\inlinecode}{on line, boxrule=0pt, boxsep=0pt, top=2pt, left=2pt, bottom=2pt, right=2pt, colback=gray!15, colframe=white, fontupper={\\ttfamily \\footnotesize}}\r\\begin{document}\rThis is a sample of some inline code: \\inlinecode{int x = 0;}.\r\\end{document} Remove the space on the left hand of the beginning of a paragraph \\noindent Typeseeting programming packages/libraries in LaTex \\textsc{} "},{"id":10,"href":"/posts/blog/shell/","title":"Bash Scripts","section":"Blog","content":"\rBash Scripts\r#\rHow to discard changes in VScode?\r#\rNavigate to reporsitory (has .github folder) with changes. Then, git restore \u0026lt;file\u0026gt; Activate conda environment in bash script\r#\rsource ~/miniconda3/etc/profile.d/conda.sh conda activate ENV SBATCH Array jobs\r#\r#SBATCH -J array-job\t#SBATCH -a 1-3 #SBATCH -o array-job.%A.%a.log id_list=\u0026#34;./id_list.txt\u0026#34; id=`head -n $SLURM_ARRAY_TASK_ID $id_list | tail -n 1` python3 script.py -in $id srun gpu\r#\rsrun -N 1 --ntasks-per-node 2 -p v100 -q gpu --gres=gpu:1 --mem=50G --pty /bin/bash run scripts\r#\rsbatch test.slm check sequence process\r#\rsqueue -u username cancel sequence\r#\rscancel jobID copy files into another folder\r#\rcp ./*.pdb ../newfoler count files in a directory in command line\r#\rcd directory ls -l | wc -l "},{"id":11,"href":"/posts/proteomics/pythonblocks/","title":"commonly used python codes","section":"Proteomics","content":"\runzip compressed files into a new directory\r#\rgz_file = os.path.join(root, file) out_file = \u0026#34;new_path/\u0026#34; + file with gzip.open(gz_file,\u0026#34;rb\u0026#34;) as f_in, open(out_file,\u0026#34;wb\u0026#34;) as f_out: shutil.copyfileobj(f_in, f_out) copy and move\r#\rimport shutil shutil.copyfile(\u0026#34;old_path/file.txt\u0026#34;, \u0026#34;new_path/file.txt\u0026#34;) "},{"id":12,"href":"/posts/proteomics/command/","title":"commonly used Shell script and Command line, when working on large dataset in HPC","section":"Proteomics","content":"ls -l | wc -l wget -i links.txt -o wget.log tar -tzf file.tar.gz | wc -l # count files in tar.gz without unzip # it actually counts lines, and it will cause counting \\n as a line tar -zcvf myfolder.tar.gz myfolder cp ./folder/*.txt ./newfolder gunzip *.gz # for f in *.gz; do gunzip \u0026#34;$f\u0026#34;; done cat *.fa \u0026gt; all_data.fa # concatenate multiple fasta files into one fasta file in command line source ~/miniconda3/etc/profile.d/conda.sh conda activate SE3nv PDBdb_dir_has=() for pdb_fp in $PDBdb_dir*pdb; do pdb=$(basename $pdb_fp .pdb) PDBdb_dir_has+=($pdb) done A=() lines=`cat $AFDB_pLDDT_70_log` for line in $lines; do if [[ $line == *\u0026#34;.pdb\u0026#34; ]]; then entry=$(echo \u0026#34;$line\u0026#34; | cut -d \u0026#34;.\u0026#34; -f 1) A+=($entry) fi done "},{"id":13,"href":"/tools/","title":"Contact","section":"Introduction","content":"\rContact\r#\rOutlook: LHLLIHUILIN@outlook.com UniEmail: lihuilin@westlake.edu.cn Gmail: lihuilin023@gmail.com "},{"id":14,"href":"/posts/proteomics/database/","title":"databases: PDB, Swiss-Prot","section":"Proteomics","content":"\rPDB database\r#\rdownload\r#\rhttps://files.rcsb.org/pub/pdb/data/structures/all/pdb/ save this website as PDB - FTP Archive over HTTP.html extract pdbXXXX.ent.gz list from bs4 import BeautifulSoup # how many ent.gz file with open(\u0026#39;PDB - FTP Archive over HTTP.html\u0026#39;, \u0026#39;r\u0026#39;) as file: html_content = file.read() soup = BeautifulSoup(html_content, \u0026#39;lxml\u0026#39;) text = soup.get_text(\u0026#39;\\n\u0026#39;, \u0026#39;\\n\\n\u0026#39;) lines = text.split(\u0026#39;\\n\u0026#39;) PDB_id_list = [] for line in lines: if \u0026#39;ent.gz\u0026#39; in line: PDB_id_list.append(line.split(\u0026#34;.\u0026#34;)[0][4:] #.split(\u0026#34;pdb\u0026#34;)[1]) # PDB_id_list[:5] # [\u0026#39;100d\u0026#39;, \u0026#39;101d\u0026#39;, \u0026#39;101m\u0026#39;, \u0026#39;102d\u0026#39;, \u0026#39;102l\u0026#39;] Is\u0026rsquo;s not okay to extract id by .split(\u0026quot;pdb\u0026quot;). Because pdb might be also the part of the id in some special cases, for example, pdb1pdb.ent.gz.\rparallel downloads # split into 44 entry_i.txt file length = 5000 count = len(df)//length for i in range(43): subdf = df.iloc[i*5000:i*5000+5000] sublist =subdf[\u0026#34;id\u0026#34;].tolist() string = \u0026#34;,\u0026#34;.join(sublist) text = open(\u0026#39;groups/entry_\u0026#39;+ str(i)+\u0026#39;.txt\u0026#39;, \u0026#39;w\u0026#39;) text.write(string) text.close() final_sublist = df.iloc[43*5000:43*5000+5000][\u0026#34;id\u0026#34;].tolist() finalstring = \u0026#34;,\u0026#34;.join(final_sublist) finaltext = open(\u0026#39;groups/entry_\u0026#39;+ str(43)+\u0026#39;.txt\u0026#39;, \u0026#39;w\u0026#39;) finaltext.write(finalstring) finaltext.close() Obatin the batch-download script batch_download.sh from Batch Downloads with Shell Script\n#!/bin/bash cd /(your path) for line in $(cat ./../groups/entry_list.txt); do ./../batch_download.sh -f ./../groups/${line} -p \u0026amp; done # while IFS= read -r line; do # ./../batch_download.sh -f ./../groups/${line} -p \u0026amp; # done \u0026lt; \u0026lt;(grep \u0026#34;\u0026#34; ./../groups/entry_list.txt) where entry_list.txt has entry_i.txt line by line.\n$ bash parallel_download.sh \u0026gt; download.log Failed download is common when Shell script downloads large files simutaneously. Therefore, it is important to check whether the script has downloaded the complete and correct ent.gz files. For example, by gunzip *gz, the wrong .gz files will output unzip error. And then, this wrong .gz file should be removed and we need to download it again.\rdesciption\r#\rAt this moment, I download 218,546 PDB entries from PDB database.\nSwiss-Prot database\r#\rdownload\r#\rUniProt provides the reviewed Swiss-Prot database. Fig.1 the Entry from UniProt\rWith the help of unipressed, I can quickly analyze the reviewed Swiss-Prot database. There are 22,140 enries whose 3D structure information are not stored. In the remaining 549,724 entries who have 3D structures, 33,725 entries have 3D structure from both Alphafold Protein Structure Database and PDB database; 2,194 entries only have 3D structure from PDB database; 513,805 entries only have 3D structure from Alphafold Protein Structure Database.\nI compared the dataset that is already compressed in Alphafold Protein Structure Database (Fig.2) and the 513,805 entries who only have Alphafold Protein Structure Database 3D structure as recorded in UniProt (Fig.1). 4,274 entries\u0026rsquo; structures in (Fig.1) have not been stored in (Fig.2).\nTo minimize duplications between the UniProt Swiss-Prot database and the PDB database, I focused on downloading the structures of 513,805 entries from UniProt (Swiss-Prot) that are only available in the Alphafold Protein Structure Database. Therefore, I only need to download 4,274 entries\u0026rsquo; structures, because the remaining 509,531 entries\u0026rsquo; structures can be directly extracted from (Fig.2).\rFig.2 compressed prediction files for Swiss-Prot from Alphafold Protein Structure Database\rdescription\r#\rIn UniProt, Swiss-Prot has 571,864 entries with its corresponding fasta file. 549,724 entries have 3D struture. And most (513,805) of these structure are from AlphaFold prediction. wget https://alphafold.ebi.ac.uk/files/AF-A0A8I6A2H6-F1-model_v4.pdb\nhttps://alphafold.ebi.ac.uk/files/AF-Q4U4S6-F1-model_v4.pdb\nhttps://alphafold.ebi.ac.uk/files/AF-A0A6N3IN21-F1-model_v4.pdb\n"},{"id":15,"href":"/posts/blog/excel/","title":"Excel commands","section":"Blog","content":" In one row, concat each cell into one cell as string =CONCAT(A1:D1) "},{"id":16,"href":"/posts/proteomics/fa2mutatedfa/","title":"fasta 2 mutated fasta (python code)","section":"Proteomics","content":"import copy mutations = [\u0026#34;K114Q\u0026#34;, \u0026#34;R196I\u0026#34;, \u0026#34;Y20F\u0026#34;, \u0026#34;K8Q\u0026#34;] fa_header = open(\u0026#34;./xxxx.fasta\u0026#34;, \u0026#34;r\u0026#34;).read().split(\u0026#34;\\n\u0026#34;)[0] fa_seq = open(\u0026#34;./xxxx.fasta\u0026#34;, \u0026#34;r\u0026#34;).read().split(\u0026#34;\\n\u0026#34;)[1] new_seq = copy.copy(fa_seq) for mutation in mutations: orig = mutation[0] idx = int(mutation[1:-1]) mut = mutation[-1] seq_list = list(new_seq) if seq_list[idx-1] == orig: seq_list[idx-1] = mut else: print(\u0026#34;Check!\u0026#34;) new_seq = \u0026#39;\u0026#39;.join(seq_list) with open(\u0026#39;mutatedfa.fa\u0026#39;, \u0026#39;w\u0026#39;) as f: f.write(fa_header) f.write(\u0026#34;\\n\u0026#34;) f.write(new_seq) "},{"id":17,"href":"/posts/proteomics/fasimilarity/","title":"fasta similarity","section":"Proteomics","content":"\rimport parasail\r#\rresult = parasail.sw_trace_scan_16(\u0026#34;asdf\u0026#34;, \u0026#34;asdf\u0026#34;, 11, 1, parasail.blosum62) "},{"id":18,"href":"/posts/proteomics/files/","title":"files list","section":"Proteomics","content":"\rlist files in a folder\r#\rimport glob files = glob.glob(\u0026#39;CSV/*.csv\u0026#39;) # [\u0026#39;CSV\\\\x_109.csv\u0026#39;, \u0026#39;CSV\\\\x_116.csv\u0026#39;, \u0026#39;CSV\\\\x_121.csv\u0026#39;, ... ] #\r"},{"id":19,"href":"/posts/proteomics/foldseek/","title":"foldseek usage (straightforward)","section":"Proteomics","content":" foldseek github Foldseek User Guide Installation: Conda installer (Linux and macOS)\r#\rconda install -c conda-forge -c bioconda foldseek Customize Database\r#\rfolsseek createdb myDB_dir/ myDB\r# myDB_dir has lots of pdb files foldseek easy-search\r#\rusage: foldseek easy-search \u0026lt;i:PDB|mmCIF[.gz]\u0026gt; ... \u0026lt;i:PDB|mmCIF[.gz]\u0026gt;|\u0026lt;i:stdin\u0026gt; \u0026lt;i:targetFastaFile[.gz]\u0026gt;|\u0026lt;i:targetDB\u0026gt; \u0026lt;o:alignmentFile\u0026gt; \u0026lt;tmpDir\u0026gt; [options]\rBy Martin Steinegger \u0026lt;martin.steinegger@snu.ac.kr\u0026gt;\roptions: prefilter:\r--comp-bias-corr INT Correct for locally biased amino acid composition (range 0-1) [1]\r--comp-bias-corr-scale FLOAT Correct for locally biased amino acid composition (range 0-1) [1.000]\r--seed-sub-mat TWIN Substitution matrix file for k-mer generation [aa:3di.out,nucl:3di.out]\r-s FLOAT Sensitivity: 1.0 faster; 4.0 fast; 7.5 sensitive [9.500]\r-k INT k-mer length (0: automatically set to optimum) [6]\r--target-search-mode INT target search mode (0: regular k-mer, 1: similar k-mer) [0]\r--k-score TWIN k-mer threshold for generating similar k-mer lists [seq:2147483647,prof:2147483647]\r--max-seqs INT Maximum results per query sequence allowed to pass the prefilter (affects sensitivity) [1000]\r--split INT Split input into N equally distributed chunks. 0: set the best split automatically [0]\r--split-mode INT 0: split target db; 1: split query db; 2: auto, depending on main memory [2]\r--split-memory-limit BYTE Set max memory per split. E.g. 800B, 5K, 10M, 1G. Default (0) to all available system memory [0]\r--diag-score BOOL Use ungapped diagonal scoring during prefilter [1]\r--exact-kmer-matching INT Extract only exact k-mers for matching (range 0-1) [0]\r--mask INT Mask sequences in k-mer stage: 0: w/o low complexity masking, 1: with low complexity masking [0]\r--mask-prob FLOAT Mask sequences is probablity is above threshold [1.000]\r--mask-lower-case INT Lowercase letters will be excluded from k-mer search 0: include region, 1: exclude region [1]\r--min-ungapped-score INT Accept only matches with ungapped alignment score above threshold [30]\r--spaced-kmer-mode INT 0: use consecutive positions in k-mers; 1: use spaced k-mers [1]\r--spaced-kmer-pattern STR User-specified spaced k-mer pattern []\r--local-tmp STR Path where some of the temporary files will be created []\r--exhaustive-search BOOL Turns on an exhaustive all vs all search by by passing the prefilter step [0]\ralign:\r--min-seq-id FLOAT List matches above this sequence identity (for clustering) (range 0.0-1.0) [0.000]\r-c FLOAT List matches above this fraction of aligned (covered) residues (see --cov-mode) [0.000]\r--cov-mode INT 0: coverage of query and target\r1: coverage of target\r2: coverage of query\r3: target seq. length has to be at least x% of query length\r4: query seq. length has to be at least x% of target length\r5: short seq. needs to be at least x% of the other seq. length [0]\r--max-rejected INT Maximum rejected alignments before alignment calculation for a query is stopped [2147483647]\r--max-accept INT Maximum accepted alignments before alignment calculation for a query is stopped [2147483647]\r-a BOOL Add backtrace string (convert to alignments with mmseqs convertalis module) [0]\r--sort-by-structure-bits INT sort by bits*sqrt(alnlddt*alntmscore) [1]\r--alignment-mode INT How to compute the alignment:\r0: automatic\r1: only score and end_pos\r2: also start_pos and cov\r3: also seq.id [3]\r--alignment-output-mode INT How to compute the alignment:\r0: automatic\r1: only score and end_pos\r2: also start_pos and cov\r3: also seq.id\r4: only ungapped alignment\r5: score only (output) cluster format [0]\r-e DOUBLE List matches below this E-value (range 0.0-inf) [1.000E+01]\r--min-aln-len INT Minimum alignment length (range 0-INT_MAX) [0]\r--seq-id-mode INT 0: alignment length 1: shorter, 2: longer sequence [0]\r--alt-ali INT Show up to this many alternative alignments [0]\r--gap-open TWIN Gap open cost [aa:10,nucl:10]\r--gap-extend TWIN Gap extension cost [aa:1,nucl:1]\rprofile:\r--num-iterations INT Number of iterative profile search iterations [1]\rmisc:\r--tmscore-threshold FLOAT accept alignments with a tmsore \u0026gt; thr [0.0,1.0] [0.000]\r--tmalign-hit-order INT order hits by 0: (qTM+tTM)/2, 1: qTM, 2: tTM, 3: min(qTM,tTM) 4: max(qTM,tTM) [0]\r--tmalign-fast INT turn on fast search in TM-align [1]\r--lddt-threshold FLOAT accept alignments with a lddt \u0026gt; thr [0.0,1.0] [0.000]\r--alignment-type INT How to compute the alignment:\r0: 3di alignment\r1: TM alignment\r2: 3Di+AA [2]\r--exact-tmscore INT turn on fast exact TMscore (slow), default is approximate [0]\r--prefilter-mode INT prefilter mode: 0: kmer/ungapped 1: ungapped, 2: nofilter [0]\r--cluster-search INT first find representative then align all cluster members [0]\r--mask-bfactor-threshold FLOAT mask residues for seeding if b-factor \u0026lt; thr [0,100] [0.000]\r--input-format INT Format of input structures:\r0: Auto-detect by extension\r1: PDB\r2: mmCIF\r3: mmJSON\r4: ChemComp\r5: Foldcomp [0]\r--file-include STR Include file names based on this regex [.*]\r--file-exclude STR Exclude file names based on this regex [^$]\r--format-mode INT Output format:\r0: BLAST-TAB\r1: SAM\r2: BLAST-TAB + query/db length\r3: Pretty HTML\r4: BLAST-TAB + column headers\r5: Calpha only PDB super-posed to query\rBLAST-TAB (0) and BLAST-TAB + column headers (4)support custom output formats (--format-output)\r(5) Superposed PDB files (Calpha only) [0]\r--format-output STR Choose comma separated list of output columns from: query,target,evalue,gapopen,pident,fident,nident,qstart,qend,qlen\rtstart,tend,tlen,alnlen,raw,bits,cigar,qseq,tseq,qheader,theader,qaln,taln,mismatch,qcov,tcov\rqset,qsetid,tset,tsetid,taxid,taxname,taxlineage,\rlddt,lddtfull,qca,tca,t,u,qtmscore,ttmscore,alntmscore,rmsd,prob\rcomplexqtmscore,complexttmscore,complexu,complext,complexassignid\r[query,target,fident,alnlen,mismatch,gapopen,qstart,qend,tstart,tend,evalue,bits]\r--greedy-best-hits BOOL Choose the best hits greedily to cover the query [0]\rcommon:\r--db-load-mode INT Database preload mode 0: auto, 1: fread, 2: mmap, 3: mmap+touch [0]\r--threads INT Number of CPU-cores used (all by default) [40]\r-v INT Verbosity level: 0: quiet, 1: +errors, 2: +warnings, 3: +info [3]\r--sub-mat TWIN Substitution matrix file [aa:3di.out,nucl:3di.out]\r--max-seq-len INT Maximum sequence length [65535]\r--compressed INT Write compressed output [0]\r--remove-tmp-files BOOL Delete temporary files [1]\r--mpi-runner STR Use MPI on compute cluster with this MPI command (e.g. \u0026#34;mpirun -np 42\u0026#34;) []\r--force-reuse BOOL Reuse tmp filse in tmp/latest folder ignoring parameters and version changes [0]\r--prostt5-model STR Path to ProstT5 model []\rexpert:\r--zdrop INT Maximal allowed difference between score values before alignment is truncated (nucleotide alignment only) [40]\r--taxon-list STR Taxonomy ID, possibly multiple values separated by \u0026#39;,\u0026#39; []\r--chain-name-mode INT Add chain to name:\r0: auto\r1: always add\r[0]\r--write-mapping INT write _mapping file containing mapping from internal id to taxonomic identifier [0]\r--coord-store-mode INT Coordinate storage mode:\r1: C-alpha as float\r2: C-alpha as difference (uint16_t) [2]\r--write-lookup INT write .lookup file containing mapping from internal id, fasta id and file number [1]\r--db-output BOOL Return a result DB instead of a text file [0]\rexamples:\r# Search a single/multiple PDB file against a set of PDB files\rfoldseek easy-search examples/d1asha_ examples/ result.m8 tmp\r# Format output differently\rfoldseek easy-search examples/d1asha_ examples/ result.m8 tmp --format-output query,target,qstart,tstart,cigar\r# Align with TMalign (global)\rfoldseek easy-search examples/d1asha_ examples/ result.m8 tmp --alignment-type 1\r# Skip prefilter and perform an exhaustive alignment (slower but more sensitive)\rfoldseek easy-search examples/d1asha_ examples/ result.m8 tmp --exhaustive-search 1 "},{"id":20,"href":"/posts/blog/hugo/","title":"Hugo commands","section":"Blog","content":"Some commands and issues you might want to know when building the website using Hugo, including inserting images and resize and center images,adding emoji, travel map, linking to pages and titles, and so on.\nSUMMARY\r#\ruse themes as your own repositories insert and resize images center images add emoji add travel map Why my local website doesn\u0026rsquo;t update after I update my markdown files? link pages and tiles Add Giscus comment in Hugo-book theme Remove git in themes, and use themes as your own repositories.\r#\rgit rm --cached hugo-book -f So that, we could modify, git add, and git push the theme repository as our own.\nHow to insert images and resize images?\r#\rThe file tree is like\n├─content\r│ │ _index.md\r│ └─posts\r│ │ hugo.md\r│ │ _index.md\r│ └─images\r│ Nice.png In hugo.md file, we add Nice.png and scale it by\n{{\u0026lt; figure src=\u0026#34;../images/Nice.png\u0026#34; width=\u0026#34;400\u0026#34; alt=\u0026#34; \u0026#34; \u0026gt;}} Why we need to add ../ in front of images folder, altough hugo.md file and images folder are at the same level?\nBecause Hugo system sees hugo.md as a foler too. However, Hugo system sees _index.md as a file! Therefore, If we want to insert iamges in _index.md file, the command will be like:\n{ {\u0026lt; figure src=\u0026#34;../images/Nice.png\u0026#34; width=\u0026#34;400\u0026#34; alt=\u0026#34; \u0026#34; \u0026gt;} } Center the image?\r#\r\u0026lt;center\u0026gt;{ {\u0026lt;figure src=\u0026#34;../images/Nice.png\u0026#34; width=\u0026#34;400\u0026#34; alt=\u0026#34; \u0026#34;\u0026gt;} }\u0026lt;/center\u0026gt; At the same time, in hugo.toml, add\n[markup] [markup.goldmark] [markup.goldmark.renderer] unsafe = true How to add emoji?\r#\rDirectly copy emoji icon 👋 and paste to .md file, instead of using :wave:.\nEmoji Copy\u0026amp;Paste: https://emojidb.org/number-emojis\nHow to add a travel map in Hugo?\r#\rRefer to this article NOTE:\nIn { {\u0026lt; openstreetmap mapName=\u0026quot;\u0026lt;your map name\u0026gt;\u0026quot; \u0026gt;} }, the \u0026lt;your map name\u0026gt; is not the name you give to your map, but the name in the website link of your map. For example, I created a map whose name is Travel, I need to use travel_1036974 in the hugo shortcode.\n\u0026lt;img src=\u0026quot;../images/map.jpeg\u0026quot; alt=\u0026quot;umap\r\u0026quot; /\u0026gt;\rWhy my local website doesn\u0026rsquo;t update after I update my markdown files?\r#\rGood question. I also find my local website does not update although I have save my changes. I am not clear the excat reason, but It works well if I Press Ctrl+C to stop the local host, and hugo server again, I can see the updated website in http://localhost:1313/.\nHow to link pages and tiles?\r#\rI am in now.md file now, and there is another page.md which has #Title I want to link. I want to link to page.md and link to #Title I want to link. In page.md file, add an anchor:\n# Title I want to link {#anchor} In now.md file, add commands:\n[text] ( {{\u0026lt; ref \u0026#34;./pages.md\u0026#34; \u0026gt;}} ) # link to one page [text] ( {{\u0026lt; ref \u0026#34;./pages.md#anchor\u0026#34; \u0026gt;}} ) # link to the title Add Giscus comment in Hugo-book theme\r#\rCreate a new public github repository, e.g. site-comment. Enable the discussions feature. Install the giscus app For example,\nConfigure giscus codeblock. Go to Giscus App, and get the anable Giscus. 5. Add the script to themes\\hugo-book\\layouts\\partials\\docs\\comments.html, like this\rbookComments: false controls whether you want to display comment in this page. References\r#\rhugo book demo 【Hugo】hugo-book主题使用 hugo博客 文内插入图片 How to render markdown url with .md to correct link Linking pages in Hugo/markdown Adding comments system to a Hugo site using Giscus "},{"id":21,"href":"/posts/proteomics/parasailc/","title":"parasail in C usage (straightforward)","section":"Proteomics","content":"I want to use its Tracebacks functions, however, this function has not been involved in Python version. Therefore, I am exploring its C usage.\ninstallation (\u0026ldquo;building from a git clone\u0026rdquo;)\r#\rInstallation instructions on the parasail Github\rstraightforward steps:\ncd /storage/lihuilin/ git clone https://github.com/jeffdaily/parasail cd parasail autoreconf -fi output\rlibtoolize: putting auxiliary files in AC_CONFIG_AUX_DIR, \u0026#39;build-aux\u0026#39;.\rlibtoolize: copying file \u0026#39;build-aux/ltmain.sh\u0026#39;\rlibtoolize: putting macros in AC_CONFIG_MACRO_DIRS, \u0026#39;m4\u0026#39;.\rlibtoolize: copying file \u0026#39;m4/libtool.m4\u0026#39;\rlibtoolize: copying file \u0026#39;m4/ltoptions.m4\u0026#39;\rlibtoolize: copying file \u0026#39;m4/ltsugar.m4\u0026#39;\rlibtoolize: copying file \u0026#39;m4/ltversion.m4\u0026#39;\rlibtoolize: copying file \u0026#39;m4/lt~obsolete.m4\u0026#39;\rconfigure.ac:109: installing \u0026#39;build-aux/compile\u0026#39;\rconfigure.ac:78: installing \u0026#39;build-aux/config.guess\u0026#39;\rconfigure.ac:78: installing \u0026#39;build-aux/config.sub\u0026#39;\rconfigure.ac:70: installing \u0026#39;build-aux/install-sh\u0026#39;\rconfigure.ac:70: installing \u0026#39;build-aux/missing\u0026#39;\rMakefile.am: installing \u0026#39;build-aux/depcomp\u0026#39;\rparallel-tests: installing \u0026#39;build-aux/test-driver\u0026#39; mkdir -p /storage/lihuilin/parasail/local ./configure --prefix=/storage/lihuilin/parasail/local/parasail output\r... ...\r-=-=-=-=-=-=-=-=-=-= Configuration Complete =-=-=-=-=-=-=-=-=-=-=-\rConfiguration summary :\rparasail version : .................... 2.6.2\rHost CPU : ............................ x86_64\rHost Vendor : ......................... pc\rHost OS : ............................. linux-gnu\rToolchain :\rCC : .................................. gcc (gnu, 8.3.1)\rCXX : ................................. g++ (gnu, 8.3.1)\rFlags :\rCFLAGS : .............................. -g -O2\rCXXFLAGS : ............................ -g -O2\rCPPFLAGS : ............................\rLDFLAGS : .............................\rLIBS : ................................\rIntrinsics :\rSSE2 : ................................ auto (yes)\rSSE2_CFLAGS : .........................\rSSE4.1 : .............................. auto (yes)\rSSE41_CFLAGS : ........................ -msse4.1\rAVX2 : ................................ auto (yes)\rAVX2_CFLAGS : ......................... -mavx2\rAVX512 : .............................. auto (yes)\rAVX512F_CFLAGS : ...................... -mavx512f\rAVX512BW_CFLAGS : ..................... -mavx512bw\rAVX512VBMI_CFLAGS : ................... -mavx512vbmi\rAltivec : ............................. auto (no)\rALTIVEC_CFLAGS : ...................... not supported\rARM NEON : ............................ auto (no)\rNEON_CFLAGS : ......................... not supported\rEXTRA_NEON_CFLAGS : ................... -fopenmp-simd -DSIMDE_ENABLE_OPENMP\rDependencies :\rOPENMP_CFLAGS : ....................... -fopenmp\rOPENMP_CXXFLAGS : ..................... -fopenmp\rCLOCK_LIBS : ..........................\rMATH_LIBS : ........................... -lm\rZ_CFLAGS : ............................\rZ_LIBS : .............................. -lz\rInstallation directories :\rProgram directory : ................... /storage/lihuilin/parasail/local/parasail/bin\rLibrary directory : ................... /storage/lihuilin/parasail/local/parasail/lib\rInclude directory : ................... /storage/lihuilin/parasail/local/parasail/include\rPkgconfig directory : ................. /storage/lihuilin/parasail/local/parasail/lib/pkgconfig\rCompiling some other packages against parasail may require\rthe addition of \u0026#39;/storage/lihuilin/parasail/local/parasail/lib/pkgconfig\u0026#39; to the\rPKG_CONFIG_PATH environment variable. make output\rmake all-am\rmake[1]: Entering directory \u0026#39;/storage/lihuilin/parasail\u0026#39;\r... ...\rmake[1]: Leaving directory \u0026#39;/storage/lihuilin/parasail\u0026#39; make install output\rmake[1]: Entering directory \u0026#39;/storage/lihuilin/parasail\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/lib\u0026#39;\r/bin/sh ./libtool --mode=install /usr/bin/install -c libparasail.la \u0026#39;/storage/lihuilin/parasail/local/parasail/lib\u0026#39;\rlibtool: install: /usr/bin/install -c .libs/libparasail.so.8.1.2 /storage/lihuilin/parasail/local/parasail/lib/libparasail.so.8.1.2\rlibtool: install: (cd /storage/lihuilin/parasail/local/parasail/lib \u0026amp;\u0026amp; { ln -s -f libparasail.so.8.1.2 libparasail.so.8 || { rm -f libparasail.so.8 \u0026amp;\u0026amp; ln -s libparasail.so.8.1.2 libparasail.so.8; }; })\rlibtool: install: (cd /storage/lihuilin/parasail/local/parasail/lib \u0026amp;\u0026amp; { ln -s -f libparasail.so.8.1.2 libparasail.so || { rm -f libparasail.so \u0026amp;\u0026amp; ln -s libparasail.so.8.1.2 libparasail.so; }; })\rlibtool: install: /usr/bin/install -c .libs/libparasail.lai /storage/lihuilin/parasail/local/parasail/lib/libparasail.la\rlibtool: install: /usr/bin/install -c .libs/libparasail.a /storage/lihuilin/parasail/local/parasail/lib/libparasail.a\rlibtool: install: chmod 644 /storage/lihuilin/parasail/local/parasail/lib/libparasail.a\rlibtool: install: ranlib /storage/lihuilin/parasail/local/parasail/lib/libparasail.a\rlibtool: finish: PATH=\u0026#34;/home/lihuilin/miniconda3/condabin:/home/lihuilin/.local/bin:/home/lihuilin/bin:/opt/slurm/sbin:/opt/slurm/bin:/soft/modules/modules-4.7.0/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin:/home/lihuilin/software/silent_tools:/sbin\u0026#34; ldconfig -n /storage/lihuilin/parasail/local/parasail/lib\r----------------------------------------------------------------------\rLibraries have been installed in:\r/storage/lihuilin/parasail/local/parasail/lib\rIf you ever happen to want to link against installed libraries\rin a given directory, LIBDIR, you must either use libtool, and\rspecify the full pathname of the library, or use the \u0026#39;-LLIBDIR\u0026#39;\rflag during linking and do at least one of the following:\r- add LIBDIR to the \u0026#39;LD_LIBRARY_PATH\u0026#39; environment variable\rduring execution\r- add LIBDIR to the \u0026#39;LD_RUN_PATH\u0026#39; environment variable\rduring linking\r- use the \u0026#39;-Wl,-rpath -Wl,LIBDIR\u0026#39; linker flag\r- have your system administrator add LIBDIR to \u0026#39;/etc/ld.so.conf\u0026#39;\rSee any operating system documentation about shared libraries for\rmore information, such as the ld(1) and ld.so(8) manual pages.\r----------------------------------------------------------------------\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/bin\u0026#39;\r/bin/sh ./libtool --mode=install /usr/bin/install -c apps/parasail_aligner apps/parasail_stats \u0026#39;/storage/lihuilin/parasail/local/parasail/bin\u0026#39;\rlibtool: install: /usr/bin/install -c apps/.libs/parasail_aligner /storage/lihuilin/parasail/local/parasail/bin/parasail_aligner\rlibtool: install: /usr/bin/install -c apps/.libs/parasail_stats /storage/lihuilin/parasail/local/parasail/bin/parasail_stats\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/include\u0026#39;\r/usr/bin/install -c -m 644 parasail.h \u0026#39;/storage/lihuilin/parasail/local/parasail/include\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/include\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail/matrices\u0026#39;\r/usr/bin/install -c -m 644 parasail/matrices/blosum100.h parasail/matrices/blosum30.h parasail/matrices/blosum35.h parasail/matrices/blosum40.h parasail/matrices/blosum45.h parasail/matrices/blosum50.h parasail/matrices/blosum55.h parasail/matrices/blosum60.h parasail/matrices/blosum62.h parasail/matrices/blosum65.h parasail/matrices/blosum70.h parasail/matrices/blosum75.h parasail/matrices/blosum80.h parasail/matrices/blosum85.h parasail/matrices/blosum90.h parasail/matrices/blosum_map.h parasail/matrices/blosumn.h parasail/matrices/dnafull.h parasail/matrices/nuc44.h parasail/matrices/pam10.h parasail/matrices/pam100.h parasail/matrices/pam110.h parasail/matrices/pam120.h parasail/matrices/pam130.h parasail/matrices/pam140.h parasail/matrices/pam150.h parasail/matrices/pam160.h parasail/matrices/pam170.h parasail/matrices/pam180.h parasail/matrices/pam190.h parasail/matrices/pam20.h parasail/matrices/pam200.h parasail/matrices/pam210.h parasail/matrices/pam220.h parasail/matrices/pam230.h parasail/matrices/pam240.h parasail/matrices/pam250.h parasail/matrices/pam260.h parasail/matrices/pam270.h parasail/matrices/pam280.h \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail/matrices\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail/matrices\u0026#39;\r/usr/bin/install -c -m 644 parasail/matrices/pam290.h parasail/matrices/pam30.h parasail/matrices/pam300.h parasail/matrices/pam310.h parasail/matrices/pam320.h parasail/matrices/pam330.h parasail/matrices/pam340.h parasail/matrices/pam350.h parasail/matrices/pam360.h parasail/matrices/pam370.h parasail/matrices/pam380.h parasail/matrices/pam390.h parasail/matrices/pam40.h parasail/matrices/pam400.h parasail/matrices/pam410.h parasail/matrices/pam420.h parasail/matrices/pam430.h parasail/matrices/pam440.h parasail/matrices/pam450.h parasail/matrices/pam460.h parasail/matrices/pam470.h parasail/matrices/pam480.h parasail/matrices/pam490.h parasail/matrices/pam50.h parasail/matrices/pam500.h parasail/matrices/pam60.h parasail/matrices/pam70.h parasail/matrices/pam80.h parasail/matrices/pam90.h parasail/matrices/pam_map.h \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail/matrices\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail\u0026#39;\r/usr/bin/install -c -m 644 parasail/cpuid.h parasail/io.h parasail/function_lookup.h parasail/matrix_lookup.h \u0026#39;/storage/lihuilin/parasail/local/parasail/include/parasail\u0026#39;\r/usr/bin/mkdir -p \u0026#39;/storage/lihuilin/parasail/local/parasail/lib/pkgconfig\u0026#39;\r/usr/bin/install -c -m 644 parasail-1.pc \u0026#39;/storage/lihuilin/parasail/local/parasail/lib/pkgconfig\u0026#39;\rmake[1]: Leaving directory \u0026#39;/storage/lihuilin/parasail\u0026#39; usage\r#\rcd /storage/lihuilin/parasail/local/parasail/bin .\r├── myseqs.fasta\r├── parasail_aligner\r└── parasail_stats myseqs.fasta is\n\u0026gt;sequence1\rCYAMYGSSTHLVLTLGDGVDGFTLDTNLGEFILTHPNLRIPPQKAIYSINEGPCDKKSPNGKLRLLYEAFPMAFLMEQAGGKAVNDRGERILDLVPSHIHDKSSIWLGSSGEIDKFLDHIGKSQ\r\u0026gt;sequence2\rIKFPNGVQKYIKFCQEEDKSTNRPYTSRYIGSLVADFHRNLLKGGIYLYPSTASHPDGKLRLLYECNPMAFLAEQAGGKASDGKERILDIIPETLHQRRSFFVGNDHMVEDVERFIREFPDA In terminal, ./parasail_aligner -f myseqs.fasta -O EMBOSS -a sw_trace_scan_16 sequence2 50 PSTASHPDGKLRLLYECNPMAFLAEQAGGKA-SDGKERILDIIPETLHQR 98\r|.....|:||||||||..|||||.||||||| :|..|||||::|..:|.:\rsequence1 53 PCDKKSPNGKLRLLYEAFPMAFLMEQAGGKAVNDRGERILDLVPSHIHDK 102\rsequence2 99 RSFFVGNDHMVEDVERFI 116\r.|.::|:. .::::|:\rsequence1 103 SSIWLGSS---GEIDKFL 117\rLength: 68\rIdentity: 33/68 (48.5%)\rSimilarity: 47/68 (69.1%)\rGaps: 4/68 ( 5.9%)\rScore: 162 ./parasail_aligner -h\r#\rusage: parasail_aligner [-a funcname] [-c cutoff] [-x] [-e gap_extend] [-o gap_open] [-m matrix] [-t threads] [-d] [-M match] [-X mismatch] [-k band size (for nw_banded)] [-l AOL] [-s SIM] [-i OS] [-v] [-V] -f file [-q query_file] [-g output_file] [-O output_format {EMBOSS,SAM,SAMH,SSW}] [-b batch_size] [-r memory_budget] [-C] [-A alphabet_aliases]\rDefaults:\rfuncname: sw_stats_striped_16\rcutoff: 7, must be \u0026gt;= 1, exact match length cutoff\r-x: if present, don\u0026#39;t use suffix array filter\rgap_extend: 1, must be \u0026gt;= 0\rgap_open: 10, must be \u0026gt;= 0\rmatrix: blosum62\r-d: if present, assume DNA alphabet ACGT\rmatch: 1, must be \u0026gt;= 0\rmismatch: 0, must be \u0026gt;= 0\rthreads: system-specific default, must be \u0026gt;= 1\rAOL: 80, must be 0 \u0026lt;= AOL \u0026lt;= 100, percent alignment length\rSIM: 40, must be 0 \u0026lt;= SIM \u0026lt;= 100, percent exact matches\rOS: 30, must be 0 \u0026lt;= OS \u0026lt;= 100, percent optimal score\rover self score\r-v: verbose output, report input parameters and timing\r-V: verbose memory output, report memory use\rfile: no default, must be in FASTA format\rquery_file: no default, must be in FASTA format\routput_file: parasail.csv\routput_format: no default, must be one of {EMBOSS,SAM,SAMH,SSW}\rbatch_size: 0 (calculate based on memory budget),\rhow many alignments before writing output\rmemory_budget: 2GB or half available from system query (100.970 GB)\r-C: if present, use case sensitive alignments, matrices, etc.\ralphabet_aliases: traceback will treat these pairs of characters as matches,\rfor example, \u0026#39;TU\u0026#39; for one pair, or multiple pairs as \u0026#39;XYab\u0026#39; "},{"id":22,"href":"/posts/proteomics/pdb2fasta/","title":"pdb 2 fasta","section":"Proteomics","content":"from Bio import SeqIO def pdb2fasta(pdb_file, fasta_file, HEADER=None): with open(pdb_file, \u0026#39;r\u0026#39;) as pdb_file: for record in SeqIO.parse(pdb_file, \u0026#39;pdb-atom\u0026#39;): if \u0026#39;?\u0026#39; in record.id: header = HEADER else: header = record.id with open(fasta_file, \u0026#39;w\u0026#39;) as fasta_file: fasta_file.write(\u0026#39;\u0026gt;\u0026#39; + header+\u0026#39;\\n\u0026#39;) fasta_file.write(str(record.seq)) return record.seq "},{"id":23,"href":"/posts/proteomics/pdb2chains/","title":"pdb 2 pdbs of chains","section":"Proteomics","content":"from Bio import PDB if __name__==\u0026#34;__main__\u0026#34;: for pdbfile_path in glob.glob(\u0026#34;/path/DB/pure_PDB/*.pdb\u0026#34;): name = pdbfile_path.split(\u0026#34;\\\\\u0026#34;)[-1].split(\u0026#34;.\u0026#34;)[0] print(name, end=\u0026#34; \u0026#34;) pdb = PDB.PDBParser().get_structure(name, pdbfile_path) pdb_io = PDB.PDBIO() pdb_chains = pdb.get_chains() for chain in pdb_chains: pdb_io.set_structure(chain) pdb_io.save(\u0026#34;/path/DB/pure_PDB_DB_by_Chain/\u0026#34; + pdb.get_id() + \u0026#34;_\u0026#34; + chain.get_id() + \u0026#34;.pdb\u0026#34;) print(\u0026#39;-- Done\u0026#39;) "},{"id":24,"href":"/posts/blog/plotlycolor/","title":"Plotly: Copy\u0026Paste Continuous and Discrete colors","section":"Blog","content":"\rDiscrete color names\r#\raliceblue\rantiquewhite\raqua\raquamarine\razure\rbeige\rbisque\rblack\rblanchedalmond\rblue\rblueviolet\rbrown\rburlywood\rcadetblue\rchartreuse\rchocolate\rcoral\rcornflowerblue\rcornsilk\rcrimson\rcyan\rdarkblue\rdarkcyan\rdarkgoldenrod\rdarkgray\rdarkgrey\rdarkgreen\rdarkkhaki\rdarkmagenta\rdarkolivegreen\rdarkorange\rdarkorchid\rdarkred\rdarksalmon\rdarkseagreen\rdarkslateblue\rdarkslategray\rdarkslategrey\rdarkturquoise\rdarkviolet\rdeeppink\rdeepskyblue\rdimgray\rdimgrey\rdodgerblue\rfirebrick\rfloralwhite\rforestgreen\rfuchsia\rgainsboro\rghostwhite\rgold\rgoldenrod\rgray\rgrey\rgreen\rgreenyellow\rhoneydew\rhotpink\rindianred\rindigo\rivory\rkhaki\rlavender\rlavenderblush\rlawngreen\rlemonchiffon\rlightblue\rlightcoral\rlightcyan\rlightgoldenrodyellow\rlightgray\rlightgrey\rlightgreen\rlightpink\rlightsalmon\rlightseagreen\rlightskyblue\rlightslategray\rlightslategrey\rlightsteelblue\rlightyellow\rlime\rlimegreen\rlinen\rmagenta\rmaroon\rmediumaquamarine\rmediumblue\rmediumorchid\rmediumpurple\rmediumseagreen\rmediumslateblue\rmediumspringgreen\rmediumturquoise\rmediumvioletred\rmidnightblue\rmintcream\rmistyrose\rmoccasin\rnavajowhite\rnavy\roldlace\rolive\rolivedrab\rorange\rorangered\rorchid\rpalegoldenrod\rpalegreen\rpaleturquoise\rpalevioletred\rpapayawhip\rpeachpuff\rperu\rpink\rplum\rpowderblue\rpurple\rred\rrosybrown\rroyalblue\rsaddlebrown\rsalmon\rsandybrown\rseagreen\rseashell\rsienna\rsilver\rskyblue\rslateblue\rslategray\rslategrey\rsnow\rspringgreen\rsteelblue\rtan\rteal\rthistle\rtomato\rturquoise\rviolet\rwheat\rwhite\rwhitesmoke\ryellow\ryellowgreen\rContinuous color names\r#\raggrnyl\ragsunset\rblackbody\rbluered\rblues\rblugrn\rbluyl\rbrwnyl\rbugn\rbupu\rburg\rburgyl\rcividis\rdarkmint\relectric\remrld\rgnbu\rgreens\rgreys\rhot\rinferno\rjet\rmagenta\rmagma\rmint\rorrd\roranges\roryel\rpeach\rpinkyl\rplasma\rplotly3\rpubu\rpubugn\rpurd\rpurp\rpurples\rpurpor\rrainbow\rrdbu\rrdpu\rredor\rreds\rsunset\rsunsetdark\rteal\rtealgrn\rturbo\rviridis\rylgn\rylgnbu\rylorbr\rylorrd\ralgae\ramp\rdeep\rdense\rgray\rhaline\rice\rmatter\rsolar\rspeed\rtempo\rthermal\rturbid\rarmyrose\rbrbg\rearth\rfall\rgeyser\rprgn\rpiyg\rpicnic\rportland\rpuor\rrdgy\rrdylbu\rrdylgn\rspectral\rtealrose\rtemps\rtropic\rbalance\rcurl\rdelta\roxy\redge\rhsv\ricefire\rphase\rtwilight\rmrybm\rmygbm\rCode\r#\rplotlycolors.ipynb\nMore\r#\rhttps://plotly.com/python/builtin-colorscales/\n"},{"id":25,"href":"/posts/blog/plane/","title":"Plotly: Drawing a plane perpendicular to a given line","section":"Blog","content":"When I am working on my binder design task, I need to select suitable hotspot residues that can lead to design better binders. In addition to hydrophobic amino acids, I also want to focus on the amino acids whose positions are close to the outside edge, and far away from the center of the protein. Therefore, I expect a plane that can approximately cut the protein layer by layer with my expected distance to the outside edge.\nBy calculating the centers of two parts of my protein, I can easily determine one line. However, it took me lots of time to find the plane that can be perpendicular to this line.\nFinally, I find the solution! 💪 TL;DR, code is here!\nA plane perpendicular to the given line\nAssuming that we want to make a plane whose vertical distance to $p_1$ is $d01$. We can determine the point $p0$ by\nfrom shapely.geometry import LineString line = LineString([p1, p2]) p0_tmp = line.interpolate(d01) p0 = np.array([p0_tmp.x, p0_tmp.y, p0_tmp.z]) 5 important points who determine the plane\nFinding $p_3$, $p_4$, $p_5$ and $p_6$ took me lots of time. I finally find that they can be found by vector calculation. For example, we want the distance between $p_0$ and $p_3$/$p_4$/$p_5$/$p_6$ is $Radius = 3$. P1 = p0 P2 = p2 Radius = 3 V3 = P1 - P2 V3 = V3 / np.linalg.norm(V3) e = np.array([0,0,0]) e[np.argmin(np.abs(V3))] = 1 V1 = np.cross(e, V3) V1 = V1 / np.linalg.norm(V3) V2 = np.cross(V3, V1) # p3 90 degree s3 = np.pi/2 p3 = P1 + Radius*( np.cos(s3)*V1 + np.sin(s3)*V2 ) # p4 180 degree s4 = np.pi p4 = P1 + Radius*( np.cos(s4)*V1 + np.sin(s4)*V2 ) # p5 270 degree s5 = np.pi*1.5 p5 = P1 + Radius*( np.cos(s5)*V1 + np.sin(s5)*V2 ) # p6 0 degree s6 = 0 p6 = P1 + Radius*( np.cos(s6)*V1 + np.sin(s6)*V2 ) Becasue three points determine one plane. We can finally make the plane by\nplane1_x, plane1_y, plane1_z = np.array([p6, p3, p5]).T fig.add_trace(go.Mesh3d(x=plane1_x, y=plane1_y, z=plane1_z, color=\u0026#34;lightgreen\u0026#34;, name = \u0026#34;\u0026#34;, hoverinfo=\u0026#34;skip\u0026#34;, opacity=0.50)) plane2_x, plane2_y, plane2_z = np.array([p4, p3, p5]).T fig.add_trace(go.Mesh3d(x=plane2_x, y=plane2_y, z=plane2_z, color=\u0026#34;lightgreen\u0026#34;, name = \u0026#34;\u0026#34;, hoverinfo=\u0026#34;skip\u0026#34;, opacity=0.50)) "},{"id":26,"href":"/posts/blog/assessment/","title":"Protein structure measurement","section":"Blog","content":"\rpLDDT\r#\r\u0026ldquo;predicted Local Distance Difference Test\u0026rdquo; is a per-residue measure of local confidence. It is scaled from 0 to 100, with higher scores indicating higher confidence and usually a more accurate prediction. ref\nPAE\r#\rPredicted Aligned Error (PAE) is a measure of how confident AlphaFold2 is in the relative position of two residues within the predicted structure. PAE is defined as the expected positional error at residue X, measured in Ångströms (Å), if the predicted and actual structures were aligned on residue Y.\rref\n#\r"},{"id":27,"href":"/posts/proteomics/readdata/","title":"read data","section":"Proteomics","content":"\rread id and seq from .fasta\r#\rfrom Bio import SeqIO with open(\u0026#39;fa.fasta\u0026#39;) as fasta_file: identifiers, seq = [], [] for seq_record in SeqIO.parse(fasta_file, \u0026#39;fasta\u0026#39;): identifiers.append(seq_record.id) seq.append(\u0026#34;\u0026#34;.join(seq_record.seq)) fa_df = pd.DataFrame() fa_df[\u0026#34;seq\u0026#34;] = seq fa_df[\u0026#34;id\u0026#34;] = identifiers pandas read\r#\rpd.read_csv(\u0026#39;result.csv\u0026#39;, header=None, index_col=0) # index_col=False "},{"id":28,"href":"/posts/proteomics/removedrna/","title":"remove DNA/RNA and O from pdb format","section":"Proteomics","content":"from Bio import PDB class rm_dna_rna_O(PDB.Select): def accept_residue(self, res): if (len(res.get_resname()) \u0026lt;3) or (res.id[0] != \u0026#34; \u0026#34;): return False else: return True if __name__==\u0026#34;__main__\u0026#34;: for pdbfile_path in glob.glob(\u0026#34;/path/PDBDB/*.pdb\u0026#34;): print(pdbfile_path, end=\u0026#34; \u0026#34;) name = pdbfile_path.split(\u0026#34;/\u0026#34;)[-1] pdb = PDB.PDBParser().get_structure(name, pdbfile_path) pdb_io = PDB.PDBIO() pdb_io.set_structure(pdb) pdb_io.save(\u0026#34;/path/PDBDB_no_drna/\u0026#34;+name, rm_dna_rna_O()) print(\u0026#39;-- Done\u0026#39;) "},{"id":29,"href":"/posts/proteomics/removehetatom/","title":"remove HETATOM from pdb format","section":"Proteomics","content":"from Bio import PDB pdb = PDB.PDBParser().get_structure(\u0026#34;2gq1\u0026#34;, \u0026#34;FBP/2gq1.pdb\u0026#34;) class ResSelect(PDB.Select): def accept_residue(self, res): if res.id[0] != \u0026#34; \u0026#34;: #\u0026gt;= start_res and res.id[1] \u0026lt;= end_res and res.parent.id == chain_id: return False else: return True pdb_io = PDB.PDBIO() pdb_io.set_structure(pdb) pdb_io.save(\u0026#34;FBP/2gq1_no_HETATOM.pdb\u0026#34;, ResSelect()) "},{"id":30,"href":"/posts/proteomics/seqio/","title":"SeqIO.parse()","section":"Proteomics","content":"from Bio import SeqIO with open(\u0026#34;fa.fasta\u0026#34;) as fa: for record in SeqIO.parse(fa, \u0026#34;fasta\u0026#34;): ID = record.id seq = record.seq description = recrod.description "},{"id":31,"href":"/posts/proteomics/uniprotana/","title":"use unipressed.UniprotkbClient analysis UniProt(Swiss-Prot) DB","section":"Proteomics","content":"We can reveal these information.\nTo minimize duplications between the UniProt Swiss-Prot database and the PDB database, I focused on downloading the structures of 513,805 entries from UniProt (Swiss-Prot) that are only available in the Alphafold Protein Structure Database. Therefore, I only need to download 4,274 entries\u0026rsquo; structures, because the remaining 509,531 entries\u0026rsquo; structures can be directly extracted from (Fig.2).\r{\u0026#39;AFDB\u0026#39;: \u0026#39;hasAFDB\u0026#39;,\r\u0026#39;PDB\u0026#39;: \u0026#39;no\u0026#39;,\r\u0026#39;annotationScore\u0026#39;: 1.0,\r\u0026#39;comments\u0026#39;: nan,\r\u0026#39;entryAudit\u0026#39;: \u0026#34;{\u0026#39;firstPublicDate\u0026#39;: \u0026#39;1988-08-01\u0026#39;, \u0026#39;lastAnnotationUpdateDate\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;2022-05-25\u0026#39;, \u0026#39;lastSequenceUpdateDate\u0026#39;: \u0026#39;1988-08-01\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;entryVersion\u0026#39;: 36, \u0026#39;sequenceVersion\u0026#39;: 1}\u0026#34;,\r\u0026#39;entryType\u0026#39;: \u0026#39;UniProtKB reviewed (Swiss-Prot)\u0026#39;,\r\u0026#39;extraAttributes\u0026#39;: \u0026#34;{\u0026#39;countByFeatureType\u0026#39;: {\u0026#39;Chain\u0026#39;: 1}, \u0026#39;uniParcId\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;UPI000013C2E9\u0026#39;}\u0026#34;,\r\u0026#39;features\u0026#39;: \u0026#34;[{\u0026#39;type\u0026#39;: \u0026#39;Chain\u0026#39;, \u0026#39;location\u0026#39;: {\u0026#39;start\u0026#39;: {\u0026#39;value\u0026#39;: 1, \u0026#34;\r\u0026#34;\u0026#39;modifier\u0026#39;: \u0026#39;EXACT\u0026#39;}, \u0026#39;end\u0026#39;: {\u0026#39;value\u0026#39;: 79, \u0026#39;modifier\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;EXACT\u0026#39;}}, \u0026#39;description\u0026#39;: \u0026#39;Putative uncharacterized protein Z\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;featureId\u0026#39;: \u0026#39;PRO_0000066556\u0026#39;}]\u0026#34;,\r\u0026#39;geneLocations\u0026#39;: nan,\r\u0026#39;genes\u0026#39;: nan,\r\u0026#39;index\u0026#39;: 860,\r\u0026#39;keywords\u0026#39;: \u0026#34;[{\u0026#39;id\u0026#39;: \u0026#39;KW-1185\u0026#39;, \u0026#39;category\u0026#39;: \u0026#39;Technical term\u0026#39;, \u0026#39;name\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;Reference proteome\u0026#39;}]\u0026#34;,\r\u0026#39;organism\u0026#39;: \u0026#34;{\u0026#39;scientificName\u0026#39;: \u0026#39;Ovis aries\u0026#39;, \u0026#39;commonName\u0026#39;: \u0026#39;Sheep\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;taxonId\u0026#39;: 9940, \u0026#39;lineage\u0026#39;: [\u0026#39;Eukaryota\u0026#39;, \u0026#39;Metazoa\u0026#39;, \u0026#39;Chordata\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;Craniata\u0026#39;, \u0026#39;Vertebrata\u0026#39;, \u0026#39;Euteleostomi\u0026#39;, \u0026#39;Mammalia\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;Eutheria\u0026#39;, \u0026#39;Laurasiatheria\u0026#39;, \u0026#39;Artiodactyla\u0026#39;, \u0026#39;Ruminantia\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;Pecora\u0026#39;, \u0026#39;Bovidae\u0026#39;, \u0026#39;Caprinae\u0026#39;, \u0026#39;Ovis\u0026#39;]}\u0026#34;,\r\u0026#39;organismHosts\u0026#39;: nan,\r\u0026#39;primaryAccession\u0026#39;: \u0026#39;P08105\u0026#39;,\r\u0026#39;proteinDescription\u0026#39;: \u0026#34;{\u0026#39;recommendedName\u0026#39;: {\u0026#39;fullName\u0026#39;: {\u0026#39;value\u0026#39;: \u0026#39;Putative \u0026#34;\r\u0026#34;uncharacterized protein Z\u0026#39;}}}\u0026#34;,\r\u0026#39;proteinExistence\u0026#39;: \u0026#39;4: Predicted\u0026#39;,\r\u0026#39;references\u0026#39;: \u0026#34;[{\u0026#39;referenceNumber\u0026#39;: 1, \u0026#39;citation\u0026#39;: {\u0026#39;id\u0026#39;: \u0026#39;6193483\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;citationType\u0026#39;: \u0026#39;journal article\u0026#39;, \u0026#39;authors\u0026#39;: [\u0026#39;Powell B.C.\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;Sleigh M.J.\u0026#39;, \u0026#39;Ward K.A.\u0026#39;, \u0026#39;Rogers G.E.\u0026#39;], \u0026#34;\r\u0026#34;\u0026#39;citationCrossReferences\u0026#39;: [{\u0026#39;database\u0026#39;: \u0026#39;PubMed\u0026#39;, \u0026#39;id\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;6193483\u0026#39;}, {\u0026#39;database\u0026#39;: \u0026#39;DOI\u0026#39;, \u0026#39;id\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;10.1093/nar/11.16.5327\u0026#39;}], \u0026#39;title\u0026#39;: \u0026#39;Mammalian keratin gene \u0026#34;\r\u0026#39;families: organisation of genes coding for the B2 high-sulphur \u0026#39;\r\u0026#34;proteins of sheep wool.\u0026#39;, \u0026#39;publicationDate\u0026#39;: \u0026#39;1983\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;journal\u0026#39;: \u0026#39;Nucleic Acids Res.\u0026#39;, \u0026#39;firstPage\u0026#39;: \u0026#39;5327\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;lastPage\u0026#39;: \u0026#39;5346\u0026#39;, \u0026#39;volume\u0026#39;: \u0026#39;11\u0026#39;}, \u0026#39;referencePositions\u0026#39;: \u0026#34;\r\u0026#34;[\u0026#39;NUCLEOTIDE SEQUENCE [GENOMIC DNA]\u0026#39;]}]\u0026#34;,\r\u0026#39;secondaryAccessions\u0026#39;: nan,\r\u0026#39;sequence\u0026#39;: \u0026#34;{\u0026#39;value\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;MSSSLEITSFYSFIWTPHIGPLLFGIGLWFSMFKEPSHFCPCQHPHFVEVVIPCDSLSRSLRLRVIVLFLAIFFPLLNI\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;length\u0026#39;: 79, \u0026#39;molWeight\u0026#39;: 9128, \u0026#39;crc64\u0026#39;: \u0026#39;A663EB489F6290C3\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;md5\u0026#39;: \u0026#39;80A5E20AFD024495655E6A9FA7B6166B\u0026#39;}\u0026#34;,\r\u0026#39;uniProtKBCrossReferences\u0026#39;: \u0026#34;[{\u0026#39;database\u0026#39;: \u0026#39;EMBL\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;X01610\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;properties\u0026#39;: [{\u0026#39;key\u0026#39;: \u0026#39;ProteinId\u0026#39;, \u0026#39;value\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;CAA25758.1\u0026#39;}, {\u0026#39;key\u0026#39;: \u0026#39;Status\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;-\u0026#39;}, \u0026#34;\r\u0026#34;{\u0026#39;key\u0026#39;: \u0026#39;MoleculeType\u0026#39;, \u0026#39;value\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;Genomic_DNA\u0026#39;}]}, {\u0026#39;database\u0026#39;: \u0026#39;PIR\u0026#39;, \u0026#39;id\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;S07912\u0026#39;, \u0026#39;properties\u0026#39;: [{\u0026#39;key\u0026#39;: \u0026#39;EntryName\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;value\u0026#39;: \u0026#39;S07912\u0026#39;}]}, {\u0026#39;database\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;AlphaFoldDB\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;P08105\u0026#39;, \u0026#39;properties\u0026#39;: \u0026#34;\r\u0026#34;[{\u0026#39;key\u0026#39;: \u0026#39;Description\u0026#39;, \u0026#39;value\u0026#39;: \u0026#39;-\u0026#39;}]}, \u0026#34;\r\u0026#34;{\u0026#39;database\u0026#39;: \u0026#39;Proteomes\u0026#39;, \u0026#39;id\u0026#39;: \u0026#39;UP000002356\u0026#39;, \u0026#34;\r\u0026#34;\u0026#39;properties\u0026#39;: [{\u0026#39;key\u0026#39;: \u0026#39;Component\u0026#39;, \u0026#39;value\u0026#39;: \u0026#34;\r\u0026#34;\u0026#39;Unplaced\u0026#39;}]}]\u0026#34;,\r\u0026#39;uniProtkbId\u0026#39;: \u0026#39;Z_SHEEP\u0026#39;} from unipressed import UniprotkbClient import pandas as pd # Entries in Uniprot(Swiss-Prot) uniprot_sp_entries_txt = open(\u0026#34;/path/uniprot_sprot.fasta/SwissProt_entries.txt\u0026#34;, \u0026#34;r\u0026#34;) uniprot_sp_entries = uniprot_sp_entries_txt.read().split(\u0026#39;\\n\u0026#39;) print(len(uniprot_sp_entries), uniprot_sp_entries[-3:]) # Split the whole entries in Uniprot(Swiss-Prot) into some chunks with the size of 1000 chunks = [uniprot_sp_entries[x:x+1000] for x in range(0, len(uniprot_sp_entries), 1000)] print(\u0026#34;We have \u0026#34;, len(chunks), \u0026#34; chunks, and the last chunk has \u0026#34;, len(chunks[-1]),\u0026#34; entries.\u0026#34;) # analyze using unipressed no_uniProtKBCrossReferences_list = [] no_uniProtKBCrossReferences_count = 0 for i in range(len(chunks)): print(i, end=\u0026#34;: \u0026#34;) chunk = chunks[i] db_dic = UniprotkbClient.fetch_many(chunk) df = pd.DataFrame(db_dic) print(\u0026#34;chunk size is \u0026#34;, len(df), end=\u0026#34;; \u0026#34;) # no_uniProtKBCrossReferences list and count no_uniProtKBCrossReferences_primaryAccession_list = df[df[\u0026#34;uniProtKBCrossReferences\u0026#34;].isna()][\u0026#34;primaryAccession\u0026#34;].tolist() no_uniProtKBCrossReferences_list.extend(no_uniProtKBCrossReferences_primaryAccession_list) no_uniProtKBCrossReferences_count = no_uniProtKBCrossReferences_count + len(no_uniProtKBCrossReferences_primaryAccession_list) print(\u0026#34;no_uniProtKBCrossReferences_count is \u0026#34;, no_uniProtKBCrossReferences_count) # has uniProtKBCrossReferences df db_df = df[~df[\u0026#34;uniProtKBCrossReferences\u0026#34;].isna()] # has PDB structure or has AFDB structure db_df.loc[:,[\u0026#34;PDB\u0026#34;]] = db_df[\u0026#34;uniProtKBCrossReferences\u0026#34;].apply(lambda x: \u0026#34;hasPDB\u0026#34; if \u0026#34;PDB\u0026#34; in pd.DataFrame(x)[\u0026#34;database\u0026#34;].tolist() else \u0026#34;no\u0026#34;) db_df.loc[:,[\u0026#34;AFDB\u0026#34;]] = db_df[\u0026#34;uniProtKBCrossReferences\u0026#34;].apply(lambda x: \u0026#34;hasAFDB\u0026#34; if \u0026#34;AlphaFoldDB\u0026#34; in pd.DataFrame(x)[\u0026#34;database\u0026#34;].tolist() else \u0026#34;no\u0026#34;) db_df.to_csv(\u0026#34;/path/UniprotkbClient_results/db_df_\u0026#34;+str(i)+\u0026#34;.csv\u0026#34;) # write no_uniProtKBCrossReferences_list to txt file no_uniProtKBCrossReferences_file = open(\u0026#34;/path/UniprotkbClient_results/no_uniProtKBCrossReferences_entries.txt\u0026#34;, \u0026#34;w\u0026#34;) for line in no_uniProtKBCrossReferences_list: no_uniProtKBCrossReferences_file.write(line+\u0026#34;\\n\u0026#34;) no_uniProtKBCrossReferences_file.close() "},{"id":32,"href":"/posts/blog/hugobookgithubaction/","title":"Website via Hugo Book and deployed by Github Action","section":"Blog","content":"\rIn Windows10, build personal website via Hugo-book and Github Actions\r#\rThis blog is about how to use Hugo to create the local website, and then deploy it on GitHub via Github Action. Everything is step by step.\nPrerequisites\r#\rGit Windows 10 VScode Github account Install Hugo\r#\rIn Windows10, right click Windows Powershell and click run as administrator. Refer to How to install chocolatey in Windows to install chocolatey. Refer to Install Hugo on Windows to install Hugo by choco install hugo-extended Make sure Hugo works well by hugo version and we will get the following\nhugo v0.123.7-312735366b20d64bd61bff8627f593749f86c964+extended windows/amd64 BuildDate=2024-03-01T16:16:06Z VendorInfo=gohugoio Create the website using Hugo themes\r#\rFor example, I want to locally work on my website in C:\\myweb folder. Still in Windows Powershell,\nNavigate back to C: driver, and create a new folder by typing mkdir myweb. Create the website by hugo new site Blogs cd Blogs git init Refer to Hugo Book Theme to use hugo-book theme by git submodule add https://github.com/alex-shpak/hugo-book themes/hugo-book Till now, if we run hugo server, we will get a plain website locally. In VS code, open Blogs folder.\nIn C:/myweb/Blogs/themes/hugo-book/exampleSite/content.en/, copy docs folder, posts folder and _index.md file to C:/myweb/Blogs/content/. Why I don\u0026rsquo;t copy the menu folder?\nBecause I found menu folder doesn\u0026rsquo;t work when I modified the website as I needed. We can customize the order of menus in leftbar by adding weight in the name.md file. For example, in each name.md file, the parameter weight determine the order of menues.\n---\rweight: 1\rtitle: One name.md file\r--- Run hugo server again, we will see a more well-done website. Deploy on Github by Action\r#\rCreate a public Github repository. For example, if this repository\u0026rsquo;s name is Blogs, your website link will be like this https://huilin-li.github.io/Blogs/\nStill in Windows Powershell, push your local work to this repository by cd C:/myweb/Blogs\rgit add .\rgit commit -m \u0026#34;update\u0026#34;\rgit branch -M main\rgit remote add origin https://github.com/Huilin-Li/Blogs.git\rgit push -u origin main Visit your GitHub repository. From the main menu choose Settings \u0026gt; Pages. In the center of your screen you will see this: Change the Source to GitHub Actions. Click Configure as the highlight in this picture: Go to Host on GitHub Pages, copy the yaml file in step 6 to Blogs/.github/workflows/hugo.yaml, and commit changes. As Step8, Step9, Step10 in Host on GitHub Pages, the deloyment is done. 🎉 Update website\r#\rAdd more contents in C:/myweb/Blogs/content/ folder, and then git push to Github repository. Github Action will automaticaly update new contents in https://huilin-li.github.io/Blogs/.\nReferences\r#\rHow to install chocolatey in Windows Hugo with Git Hub Pages on Windows Using GitHub Pages with Actions to deploy Hugo sites in seconds - Tommy Byrd // HugoConf 2022 Hugo Book Theme Host on GitHub Pages 【Hugo】hugo-book主题使用 "},{"id":33,"href":"/links/","title":"World","section":"Introduction","content":"\rWorld\r#\r"}]